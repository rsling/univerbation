% !Rnw root = ../nvuniverbation.Rnw
<<setupcorpus, cache=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\section{Analysing the usage of verb-noun units}
\label{sec:corpusbasedanalysisoftheusageofvnunits}

In this section, we apply both exploratory and confirmatory methods of analy\-sing the univerbation of N+V units using corpus data.
In Section~\ref{sub:designandchoiceofcorpus}, we lay out which research questions we aim to answer through corpus analysis, and which corpus we chose as appropriate for the task.
We then describe the sampling and annotation procedure in Section~\ref{sub:samplingandannotation}.
The results of estimating a multilevel model explaining the variation in the univerbation of N+V units are reported in Section~\ref{sub:results1multilevelmodel}.
Finally, we corroborate the multilevel model through an alternative analysis in terms of association measures in Section~\ref{sub:results2associationastrengths}, which will also partly be the basis for the experiment reported in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}.

\subsection{Design and choice of corpus}
\label{sub:designandchoiceofcorpus}

The goals of the corpus study was (i) to assess which V+N units are used in written German, (ii) to corroborate that morphosyntactic contexts, internal relations, and linking elements influence the probability of univerbation, and finally (iii) to show how strongly the individual N+V units are attracted by univerbation.
The operationalisations relied on the fact that the major graphemic principles in German are clear and dominant, and that they are both deeply rooted in diachrony and well entrenched in writers' usage (\textbf{Reference?}).
The relevant major principle for the present study was compound spelling of syntactic words, and we interpreted compound spelling as a direct indication of univerbation in the writers' grammars.

Research questions (i) and (iii) -- as opposed to (ii) -- are clearly not driven by strong specific hypotheses derived from theory, and we consequently adopted a data-driven approach with a post-hoc interpretation of the results.%
\footnote{Question (iii) only makes sense, of course, under the general hypothesis of item-specific effects in grammar from usage-based theories (\textbf{Reference?}).
Also notice that the results obtained under question (iii) were used in the choice of the stimuli for the experiment reported in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}.}
Hence, we needed to extract (close to) \textit{all} relevant N+V units from an ideally very large and varied corpus as a first step.
In a second step, we had to count their occurrences in compound and separate spelling in the relevant morphosyntactic contexts enumerated in Section~\ref{sec:theformandhistoryofnounverbunitsingerman}, viz.\ as the heads of noun phrases, in \textit{am} progressives, as participles in analytical verb forms, and as infinitives in a range of verbal constructions (for example with modal verbs).

Clearly, a large corpus with rich morphological and morphosyntactic annotations containing texts written in a broad variety of registers and styles (including ones written under low normative pressure) was required.
We chose the DECOW16B corpus \parencite{SchaeferBildhauer2012} because it fulfils all the aforementioned criteria.%
\footnote{\url{https://www.webcorpora.org}}
Much like the SketchEngine corpora \parencite{KilgarriffEa2014}, the COW corpora contain web documents from recent years.
However, the German DECOW (containing 20.5 billion tokens in 808 million sentences and 17.1 million documents) offers a much wider range of annotations compared to SketchEngine corpora, including morphological annotations and several levels of syntactic annotation (dependencies and topological parses).
For our purpose, the fully internal analysis of nominal compounds described in \textcite{SchaeferPankratz2018} was particularly of interest.
It allows for searches of roots within nominal compounds.
For example, we could query compounds with a deverbal head such as \textit{Zeitnehmen} (`time taking').
Furthermore, the interface offered by the creators of the COW corpora allows for automated queries controlled by Python scripts using the open-source SeaCOW interface.%
\footnote{\url{https://github.com/rsling/seacow}}
The scripts we used to make the queries are released on a curated open-data server along with all data as well as the \LaTeX, knitr, and R scripts created in the writing of this paper.%
\footnote{The DOI of the data set will be revealed in the accepted version of this paper.}

\subsection{Sampling and annotation}
\label{sub:samplingandannotation}

The first step of the implementation of the corpus study was the generation of a list of actually occurring N+V units.
We obtained such a list by querying for compounds with a nominal non-head and a deverbal head.
(See the scripts available under the abovementioned DOI for concrete queries and further details.)
The rationale behind this approach was that any N+V unit of interest should occur at least once in compound spelling as a fully nominalised compound.
Since this step relied on automatic annotation, the results contained erroneous results, which we removed manually.
The resulting list contained \Sexpr{nrow(concordance)} N+V units.%
\footnote{Notice that three highly frequent N+V units were excluded because they could be considered outliers, having an overly strong tendency to be used in compound spelling.
They are \textit{Teilnehmen} (``take part''), \textit{Ma√ünehmen} (``take measure''), and \textit{Teilhaben} (``have part'' = ``participate'').}

In the second step, we created lists of all relevant inflectional forms of the verb in each V+N unit and used these to query all possible compound and separate spellings (including variance in capitalisation) of each of the \Sexpr{nrow(concordance)} N+V unit types.
In total, \Sexpr{nice.int(35*nrow(concordance))} queries were executed to create the final data set used here, a number which clearly demonstrates the necessity of script-based corpus access in data-driven methods.
The queries were matched by \Sexpr{nice.int(sum(concordance$Joint))} compound spellings and \Sexpr{nice.int(sum(concordance$Separate))} separate spellings, which results in a total sample size of \Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))} tokens.

For each N+V unit in the sample, the following variables were annotated automatically: (i) the verb, (ii) the noun, (iii) whether a linking element is used in the use as a full noun, (iv) the overall frequency in the corpus.
Additionally, we manually coded all \Sexpr{nrow(concordance)} N+V units for the relation holding between the verb and the noun (see Section~\ref{sec:theformandhistoryofnounverbunitsingerman}).
The codes used in clear-cut cases were \textit{Object} (\Sexpr{length(which(concordance$Relation=="Object"))} units) and \textit{Adjunct} (\Sexpr{length(which(concordance$Relation=="Adjunct"))} units).
For \Sexpr{length(which(concordance$Relation=="Undetermined"))} units, both relations were conceivable, and those cases were coded as \textit{Undetermined}.
This class is illustrated by \textit{Daumenlutschen} (``thumb sucking''), which corresponds to the VPs in either (\ref{ex:daumenlutschen-a}) or (\ref{ex:daumenlutschen-b}).

\begin{exe}
  \ex\begin{xlist}
    \ex\gll [den Daumen]\Sub{NP_{Acc}} lutschen \\
    the thumb suck\\\label{ex:daumenlutschen-a}
    \ex\gll [am Daumen]\Sub{PP} lutschen\\
    {on the} thumb suck\\\label{ex:daumenlutschen-b}
  \end{xlist}\label{ex:daumenlutschen}
\end{exe}


\subsection{Results 1: Multilevel model}
\label{sub:results1multilevelmodel}

In this section, we present the parameter estimates (and predictions of conditional modes) for a binomial multilevel model (or generalised linear mixed model, GLMM) which models the relevant factors influencing speakers' choice of the compound and the separate spelling.%
\footnote{See \parencite{Schaefer2020a} for an overview of the method and our philosophy in modelling.}
Given the grand total of \Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))} observations in the sample (see Section~\ref{sub:samplingandannotation}), we will completely refrain from an interpretation in terms of inferential statistics.
For samples of such magnitude in data-driven approaches, frequentist significance tests are the wrong tool (\textbf{Reference?}).
Therefore, we provide standard likelihood ratio confidence intervals for parameter estimates and prediction intervals for conditional modes as an approximate measure quantifying the precision of the parameter estimates and predictions.
The models we specify reflect theoretically motivated decisions, and we therefore reject all types of model selection by means of step-up or step-down procedures.

As argued in Section~\ref{sec:thestatusofounverbunits}, we expect the probability of the univerbation of N+V units to depend on the morphosyntactic context, the relation holding between the verb and the noun, the presence of absence of a linking element in the nominal compound (as a marker of a stronger reconceptualisation) and on the specific N+V unit (a lexical tendency).
Accordingly, the response variable was chosen to be the proportion of compound spellings among all the spellings of the N+V unit.
In the input data provided to the estimator, the response variable was thus a vector of \Sexpr{nrow(concordance)} proportions, one for each N+V unit.%
\footnote{Binomial models can be specified in this manner \parencite[245--260]{ZuurEa2009}.
In the estimation of such models, the influence of each proportion is weighted according to the number of cases observed to calculate it.
Without the weighting, highly frequent observed proportions would have too small an influence on the estimation, and infrequent ones would have an inappropriately high influence.
In the case at hand, such a model on proportion data is also a convenient way of getting around the practical difficulties of estimating a model on the raw \Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))} observations.}
We specified four regressors.
The only first-level (or observation-level) fixed effect regressor is the morphosyntactic context (a four-way categorical variable).
As there is a huge number of \Sexpr{nrow(concordance)} N+V units, the lexical indicator variable for the individual N+V unit should not be used as a fixed effect \parencite[244--247]{GelmanHill2006}.
We specified a generalised linear mixed model with the N+V unit variable as a random effect.
The variables encoding the internal relation and the presence\slash absence of a linking element are nested inside the levels of the random effect, and they are therefore treated as second-level fixed effects in a multilevel model.
In R notation, the specification is shown in (\ref{eq:corpusglmmr}).%
\footnote{See Appendix~\ref{sec:specifictionofthecorpusglmm} for a precise specification in mathematical notation.}

\begin{equation}
  \mathtt{Univerbation\sim (1|NVUnit)+Context+Relation+Link} \label{eq:corpusglmmr}
\end{equation}

<<corpusglmm, results="asis", results="hide">>=

concordance.glmm$Relation <- factor(concordance.glmm$Relation, levels = c("Object", "Undetermined", "Adjunct"))
concordance.glmm$Context <- factor(concordance.glmm$Context, levels = c("Infinitive", "Participle", "NP", "Progressive"))
concordance.glmm$FullLink <- concordance.glmm$Link
concordance.glmm$Link <- concordance.glmm$Linkbinary

corpus.glmm <- glmer(cbind(Joint, Separate)~Context+Relation+Link+(1|Compound),
                     data=concordance.glmm, family=binomial,
                     na.action = na.fail, control=glmerControl(optimizer="nloptwrap2", optCtrl=list(maxfun=2e5))
                     )

corpus.fixefs <- fixef(corpus.glmm)
corpus.confints <- confint(corpus.glmm)
# corpus.confints <- cbind(corpus.fixefs, corpus.fixefs) # Fake CIs for quick recompiles.
corpus.glmm.r2 <- r.squaredGLMM(corpus.glmm)
@

<<corpusglmmreport, results="asis">>=

# Helper function.
format.ranef <- function(glmm, ranef) {
  require(lme4)
  .vcov   <- as.data.frame(VarCorr(glmm))
  list(Name = ranef, Intercept = .vcov[which(.vcov$grp == ranef), "vcov"], sd = .vcov[which(.vcov$grp == ranef), "sdcor"])
}

# Build the table.
corpus.ct <- cbind(corpus.fixefs, corpus.confints[1:7,])
colnames(corpus.ct) <- c("Estimate", "CI low", "CI high")
ranef.nv <- format.ranef(corpus.glmm, "Compound")
corpus.r2.txt <- paste0("Nakagawa \\& Schielzeth's \\CM{R^2_m=", nice.float(corpus.glmm.r2[1,1]), "} and \\CM{R^2_c=", nice.float(corpus.glmm.r2[1,2]), "}")
ranef.txt <- paste0("Random effect for V+N lemma: \\CM{Intercept=", nice.float(ranef.nv$Intercept), "}, \\CM{sd=", nice.float(ranef.nv$sd),
                    "}")
corpus.ctxt <- xtable(corpus.ct, digits = 3,
                    caption = paste0("Coefficient table for the binomial GLMM modelling the corpus data with 95\\% profile likelihood ratio confidence intervals. The horizontal line separates first-level and second-level effects. Weighting was used to account for the bias in models on proportion data. ", ranef.txt, ". The intercepts model the fixed effects Relation~=~Object and Link~=~No. " , corpus.r2.txt),
                    label = "tab:corpusglmm")

# Experimental function to fix variable names.
lme4.pretty <- function(s) {
  gsub("([a-z])([A-Z])", "\\1 = \\2", s)
}

# Print the table.
print(corpus.ctxt,
      include.rownames=T,
      floating = T,
      table.placement = '!htbp',
      booktabs = T,
      scalebox = 1,
      hline.after = c(-1,1,4,7),
      sanitize.text.function = lme4.pretty# function(x){x},
)
@

The estimated parameters of the model are given in Table~\ref{tab:corpusglmm}.
Additionally, effect plots for \textit{Context} and \textit{Relation} are given in Figure~\ref{fig:corpuseffects}\ref{fig:corpuseffects}.%
\footnote{Put in an oversimplified manner, effect plots for binomial GLM(M)s \parencite{FoxWeisberg2018} plot the probability of the outcome across values of a regressor assuming default values for all other regressors.
While model coefficients in binomial (and other) models have no direct interpretation in terms of probability, effect plots allow a more intuitive interpretation in terms of changes in probability.}
As expected, the prototypically verbal contexts (infinitives and participles in analytic verb forms) are associated with a low probability of compound spelling (the infinitive is on the intercept estimated at $\Sexpr{nice.float(corpus.ct["(Intercept)","Estimate"])}$, and participles have a coefficient of $\Sexpr{nice.float(corpus.ct["ContextParticiple","Estimate"])}$).
NPs and progressives as prototypically nominal contexts clearly favour compound spelling (coefficients of $\Sexpr{nice.float(corpus.ct["ContextNP","Estimate"])}$ and $\Sexpr{nice.float(corpus.ct["ContextProgressive","Estimate"])}$, respectively).
Both the coefficients and the effect plot (right panel in Figure~\ref{fig:corpuseffects}) show a low probability of compound spelling when the relation between the verb and the noun (on the intercept) is that of an object, and a high probability when the relation is that of an adjunct (coefficient $\Sexpr{nice.float(corpus.ct["RelationAdjunct","Estimate"])}$).
The undetermined cases are in between the two clear-cut cases (coefficient $\Sexpr{nice.float(corpus.ct["RelationUndetermined","Estimate"])}$).
The presence of a linking element in fully nominalised compounds favours compound spelling only slightly (coefficient $\Sexpr{nice.float(corpus.ct["LinkYes","Estimate"])}$).

<<corpuseffects, results="asis", fig.showtext=TRUE, fig.pos="!htbp", fig.height=4, fig.cap="Effect plots for the regressor encoding the morphosyntactic context of the N+V unit and the regressor encoding the syntactic relation within the N+V unit in the GLMM modelling the corpus data">>=
corpus.glmm.fx.context <- plot(effect("Context", corpus.glmm, KR = T),
                              rug=F, colors = c("black", "black"),
                              main="",ylab="Probability of univerbation",
                              xlab="Context"
)
corpus.glmm.fx.relation <- plot(effect("Relation", corpus.glmm, KR = T),
                                rug=F, colors = c("black", "black"),
                                main="",
                                ylab="Probability of univerbation",
                                xlab="Relation"
)
grid.arrange(corpus.glmm.fx.context, corpus.glmm.fx.relation, ncol=2)
@


<<ranefplotsetnumber, results="hide">>=
ranefplot.num=20
@

Given the narrow confidence intervals and the high marginal measure of determination $R^2_m=\Sexpr{nice.float(corpus.glmm.r2[1,1])}$, we consider the hypotheses regarding fixed effects as well corroborated by the data, especially the effects of the context and the internal relation.
Based on our commitment to a usage-based probabilistic view of language, we also predicted differences between N+V units not explainable by the fixed effects.
These effects would show up as the residual variance in the random effects (in the form of the conditional modes) not modelled by the second-level effects.
The conditional modes are centred around a second-level intercept of $\Sexpr{nice.float(ranef.nv$Intercept)}$ with a standard deviation of $\Sexpr{nice.float(ranef.nv$sd)}$.
The standard deviation is a sign that there is considerable variation between single N+V units.
Furthermore, the conditional $R^2_c$ is as high as $\Sexpr{nice.float(corpus.glmm.r2[1,2])}$.
This is commonly interpreted as saying that the fixed effects and the idiosyncratic effect of concrete N+V units almost fully explain the variance in the data.
A random selection of \Sexpr{nice.int(ranefplot.num)} conditional modes, which illustrates the relevance of lexical idiosyncrasies through obvious differences with mostly very narrow prediction intervals, is shown in Figure~\ref{fig:corpusranefs}.

<<corpusranefs, results="asis", fig.showtext=TRUE, fig.pos="!htbp", fig.cap="A random selection of conditional modes with 95\\% prediction intervals for the levels of the random effect in the GLMM modelling the corpus data">>=
set.seed(3478)
par(family = "Fira Sans")
corpus.ranef.selection <- ranef.plot(corpus.glmm, effect = "Compound", number = 20)
@

The individual V+N unit thus plays a major role in writers' tendency to univerbate V+N units.
In Section~\ref{sub:results2associationastrengths}, we approach this effect using an alternative method, and the results obtained from that method will be used to predict participants' behaviour in the controlled experiment reported in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}.

\subsection{Results 2: Association strengths}
\label{sub:results2associationastrengths}

In this section, we report an analysis of the item-specific affinities of N+V units towards univerbation.
The reason behind this additional analysis of the data is twofold.
First, we aim to demonstrate that the same interpretation can be obtained using a method that is technically much simpler and more robust against problems with the distribution of the data and against misinterpretation than multilevel modelling.
This is a valuable contribution to the current discussions in linguistics and statistics, also in the sense of methodological pluralism (see, for example \citealt{ArppeJaervikivi2007}).
Second, we saw in Section~\ref{sub:results1multilevelmodel} that the second-level predictors and the individual N+V units -- both being related to the choice of concrete N+V units -- are highly predictive of the outcome (univerbation or not).
Therefore, in the experiment reported in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}, we need to control for the N+V units' affinity towards univerbation in common language use.
The measures introduced here are ideally suited for this task.
The method we use seems superficially similar to collocational analysis (\citealt{Evert2008} for an overview) or collostructional analysis \parencite{StefanowitschGries2003}.
However, there are major differences to be explained momentarily.

We are interested in a quantification of how strongly a N+V unit tends towards univerbation vis-a-vis all other N+V units.
Thus, we need to compare the counts of cases with and without univerbation of this unit with the same counts for all other N+V units.
Such comparisons must be made relative to the overall number of the specific N+V unit as well as the number of all other units.
The required counts are nicely summarised in a 2$\times$2 contingency table as shown in Table~\ref{tab:associationsexplained}.

\begin{table}[!htbp]
  \begin{tabular}{lcc}
  \toprule
  & Univerbation & No univerbation \\
  \midrule
  Specific N+V unit   & $c_{11}$ & $c_{21}$ \\
  All other N+V units & $c_{21}$ & $c_{22}$ \\
  \bottomrule
  \end{tabular}
  \caption{2\CM{\times}2 contingency table as used in the calculation of the strengths of the associations of N+V units with univerbation}
  \label{tab:associationsexplained}
\end{table}

We're interested in deviations of the proportions between the first row and the second row, and there is a range of statistical measures for that.
One can, for example, use odds ratios or effects strengths from frequentist statistical tests.%
\footnote{p-values from frequentist statistical tests are measures of evidence, and therefore not appropriate in such situations \parencite{SchmidKuechenhoff2013,KuechenhoffSchmid2015} although they were used in early collostructional analysis.
However, even collostructional analysis is now often used with measures of effect strength \parencite{Gries2015b}.}
We chose Cram√©r's $v$ derived from standard $\chi^2$ scores ($v=\sqrt{\chi^2/n}$).
The $v$ measure quantifies for each individual N+V unit how strongly its counts (cells $c_{11}$ and $c_{21}$) deviate from its counts that we would expect if there were no difference between this unit and all other N+V units (cells $c_{21}$ and $c_{22}$) with respect to their tendency to univerbate.
Since Cram√©r's $v$ always is in the range between 0 and 1, it allows us to compare analyses where the sample size is different.
In itself, $v$ does not tell us whether the deviation is negative (for a N+V unit with less than average compound spellings) or positive (for a N+V unit with more than average compound spellings).
The information about the direction of the deviation is added by multiplying $v$ with the sign of the upper left cell of the residual table of the $\chi^2$ test.
The association scores encode almost the same information as the second-level model in the GLMM reported in Section~\ref{sub:results1multilevelmodel}, but they have a much more accessible interpretation.%
\footnote{See \citet{SchaeferPankratz2018} for a similar use of association measures.}

<<associationsall, results="asis", fig.showtext = TRUE, fig.pos="htbp", fig.height=3, fig.width=3, fig.cap=paste0("Density estimate of the distribution of the \\CM{", length(which(!is.na(concordance$all.assocs))), "} association scores (across all morphosyntactic conditions)")>>=

### Plot distributions of association measures.
density.opts <- list(lwd = 2)
par(family = "Fira Sans", mar=c(2.5, 2.5, 0.5, 2.5))

plot(density(na.omit(concordance$all.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15), ylim = c(0,42),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = ""
  )
axis(1)
axis(2)
@

We calculated the signed $v$ for each of the $\Sexpr{length(concordance$all.assocs)}$ N+V units.
Their distribution is plotted in the form of a density estimate in Figure~\ref{fig:associationsall}.%
\footnote{It approximates a scaled symmetric $\chi^2$ distribution squashed between -1 and 1.}

Based on the annotations in the corpus data set, we can also compare the association strengths for specific morphosyntactic contexts.
The counts as shown in Table~\ref{tab:associationsexplained} are simply reduced to the counts in each of the four contexts in turn.
With the resulting lower sample sizes, the $\chi^2$ measure can no longer be calculated for a number of infrequent N+V units, leading to lower $n_{Unit}$ (= the number of N+V units analysed).
The resulting distributions are shown in Figure~\ref{fig:associationssingle}.

<<associationssingle, results="asis", fig.showtext = TRUE, fig.pos="htbp", fig.height=5, fig.width=5, fig.cap="Density estimates of the distribution of the association scores in the specific morphosyntactic conditions.">>=

### Plot distributions of association measures.
density.opts <- list(lwd = 2)
par(mfrow=c(2,2), family = "Fira Sans", mar=rep(2.5, 4))

plot(density(na.omit(concordance$np.all.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15), ylim = c(0, 35),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = substitute(NPs~OB*n[Unit]*EQ*NUNIT*CB, list(EQ="=", OB="(", CB=")", NUNIT=length(which(!is.na(concordance$np.all.assocs)))))
  )
axis(1)
axis(2)

plot(density(na.omit(concordance$prog.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15), ylim = c(0, 11),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = substitute(Progressives~OB*n[Unit]*EQ*NUNIT*CB, list(EQ="=", OB="(", CB=")", NUNIT=length(which(!is.na(concordance$prog.assocs)))))
    )
axis(1)
axis(2)

plot(density(na.omit(concordance$particip.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15), ylim = c(0, 165),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = substitute(Participles~OB*n[Unit]*EQ*NUNIT*CB, list(EQ="=", OB="(", CB=")", NUNIT=length(which(!is.na(concordance$particip.assocs)))))
  )
axis(1)
axis(2)

plot(density(na.omit(concordance$infzu.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15), ylim = c(0, 165),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = substitute(Infinitives~OB*n[Unit]*EQ*NUNIT*CB, list(EQ="=", OB="(", CB=")", NUNIT=length(which(!is.na(concordance$infzu.assocs)))))
  )
axis(1)
axis(2)
@

The context-wise distributions of the association scores corroborate the results from the GLMM reported in Section~\ref{sub:results1multilevelmodel}.
In the NP context (top left panel of Figure~\ref{fig:associationssingle}), the right tail of the curve is much heavier than the left tail, which means there are mostly higher than usual tendencies towards univerbation.
In the morphosyntactically similar progressive (top right panel), the distribution is (very) approximately symmetric, but given the low number of \Sexpr{length(which(!is.na(concordance$prog.assocs)))} N+V units for which $v$ could be calculated, the result cannot be seen as stable.%
\footnote{The low number is one the one hand due to the fact that progressives are rare compared to NPs, participles, and infinitives.
On the other hand, it is likely that many N+V units cannot be used in the progressive for semantic or pragmatic reasons.}
Both prototypically verbal contexts (lower two panels) show heavier left tails, meaning that N+V units tend to resist univerbation in these contexts.
Once again, this is just another (and maybe more intuitive) look at the data in addition the GLMM analysis.

For the selection of stimuli in the experiment, the overall association strength (Figure~\ref{fig:associationsall}) is relevant, because it truly represents the effect of the unit, independently of the context.
The context effect will be controlled independently in the experiment.
To illustrate how the data analysis allows for a selection of N+V units based on their affinity towards univerbation, we show the top ten units with the highest negative and highest positive association in Table~\ref{tab:corpusassocslohi}.

<<corpuscreatetabledata, results="asis">>=
corpus.assoc.sel <- format.assocs(df = concordance, col = 'all.assocs', show.results = "all",
                               show.cols = c("Compound", "all.assocs", "Relation"),
                               num = 10, cx = "All contexts", effect = "UNIVERBATION", freq.cutoff = 0.2)
corpus.assoc.sel$Association <- nice.float(corpus.assoc.sel$Association)
corpus.assoc.sel$Relation <- revalue(corpus.assoc.sel$Relation, c("Undetermined" = "N/D"))
colnames(corpus.assoc.sel) <- c("V+N Unit", "Assoc.", "Rel.")
corpus.assoc.sel <- corpus.assoc.sel[, 1:3]
@

\begin{table}[!htbp]
  \begin{minipage}{.45\textwidth}
    \centering
    <<corpusassoctablehi, results="asis">>=
    corpus.assoc.hi <- xtable(corpus.assoc.sel[1:10,], digits = 3)
    print(corpus.assoc.hi,
          include.rownames=F,
          floating = F,
          table.placement = 'h!',
          booktabs = T,
          scalebox = 0.9,
          sanitize.text.function = function(x){x}
    )
    @
  \end{minipage}\hspace{1em}\begin{minipage}{.45\textwidth}
    \centering
    <<corpusassoctablelo, results="asis">>=
    corpus.assoc.lo <- xtable(corpus.assoc.sel[30:21,], digits = 3)
    print(corpus.assoc.lo,
          include.rownames=F,
          floating = F,
          table.placement = 'h!',
          booktabs = T,
          scalebox = 0.9,
          sanitize.text.function = function(x){x}
    )
    @
  \end{minipage}
  \caption{Top ten V+N units with a strong tendency for univerbation (left panel) and top ten V+N units with a strong tendency against univerbation (right panel)}
  \label{tab:corpusassocslohi}
\end{table}

The tables illustrate that units with the strongest tendencies against univerbation are predominantly ones with an object relation.
The ones which most strongly favour univerbation are mostly ones with an adjunct relation or an ambiguous relation.
The ten items with the least clear tendency in either direction are shown in Table~\ref{tab:corpusassoctablenull}.
They mostly have an internal object relation.

<<corpusassoctablenull, results="asis", >>=
corpus.assoc.null <- xtable(corpus.assoc.sel[11:20,], digits = 3,
                            caption = paste0("Top ten V+N units without any tendency for or against univerbation"),
                            label = "tab:corpusassoctablenull")
# Print the table.
print(corpus.assoc.null,
      include.rownames=F,
      floating = T,
      table.placement = '!htbp',
      booktabs = T,
      scalebox = 0.9,
      sanitize.text.function = function(x){x},
)
@

Among the units with an object relation, it is difficult to tell based on native-speaker intuition, why the ones in Table~\ref{tab:corpusassoctablenull} should have no preference and the ones in Table~\ref{tab:corpusassocslohi} (right panel) should resist univerbation.
This goes to show that, while we can model the tendencies to some extent using linguistic features, there are obvious item-specific effects which should be taken seriously from a theoretical perspective, and which must be accounted for in behavioural experiments.
We now turn to such an experiment in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}.


