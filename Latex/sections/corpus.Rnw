% !Rnw root = ../nvuniverbation.Rnw
<<setupcorpus, cache=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\section{Analysing the usage of noun+verb units}
\label{sec:corpusbasedanalysisoftheusageofnvunits}

In this section, we apply two quantitative methods to analyse the univerbation of N+V units using corpus data.
We motivate our choice of corpus and describe the sampling and annotation procedure in Section~\ref{sub:choiceofcorpussamplingandannotation}.
We perform exploratory analysis using association measures in Section~\ref{sub:results2associationastrengths} in order to gauge the individual tendencies of N+V units to undergo univerbation in written language usage.
Finally, the results of estimating the parameters of a generalised linear mixed model explaining the variation in the univerbation of N+V units are reported in Section~\ref{sub:results1multilevelmodel}.

\subsection{Choice of corpus, sampling, and annotation}
\label{sub:choiceofcorpussamplingandannotation}

As a first step, we adopted a data-driven approach in order to find nearly all N+V units in contemporary written usage.
In a second step, we counted their occurrences in compound and disjunct spelling in four relevant morphosyntactic contexts: fully nominalised as the heads of noun phrases, in \textit{am} progressives, as participles in analytical verb forms, and as infinitives in a range of verbal constructions).

Clearly, we required a large corpus with rich morphological and morphosyntactic annotations containing texts written in a broad variety of registers and styles (including ones written under low normative pressure).
We chose the DECOW16B corpus \parencite{SchaeferBildhauer2012a} because it fulfils all the aforementioned criteria.%
\footnote{\url{https://www.webcorpora.org}}
Much like the SketchEngine corpora \parencite{KilgarriffEa2014}, the COW corpora contain web documents from recent years.
However, the German DECOW (containing 20.5 billion tokens in 808 million sentences and 17.1 million documents) offers a much wider range of annotations compared to SketchEngine corpora, including morphological annotations and several levels of syntactic annotation (dependencies and topological parses).
For our purpose, the complete internal analysis of nominal compounds described in \citet{SchaeferPankratz2018} was particularly of interest.
This level of analysis allows for corpus searches of roots within nominal compounds.

The list of actually occurring N+V units was obtained by querying for compounds with a nominal non-head and a deverbal head.%
\footnote{See the scripts available under the following DOI for concrete queries and further details: \concealed{DOI}.}
The rationale behind this approach is that any N+V unit of interest should occur at least once in compound spelling as a fully nominalised compound.
Since this step relied on automatic annotation already available in the corpus, the results contained erroneous hits which we removed manually.
The resulting list contained $\Sexpr{nrow(concordance)}$ N+V units.%
\footnote{Three highly frequent N+V units were excluded because they could be considered outliers, as they have fully undergone lexicalisation and are virtually always used in compound spelling.
They are \textit{Teilnehmen} `to take part', \textit{Maßnehmen} `take measure', and \textit{Teilhaben} `have part' (meaning `to participate').}

In the second step, we created lists of all relevant inflectional forms of the verb in each N+V unit and used these to query all possible compound and disjunct spellings (including variance in capitalisation) of each of the $\Sexpr{nrow(concordance)}$ N+V unit types.
In total, $\Sexpr{nice.int(35*nrow(concordance))}$ queries were executed to create the final data set used here, a number which clearly demonstrates the necessity of script-based corpus access in data-driven methods.
The queries retrieved $\Sexpr{nice.int(sum(concordance$Joint))}$ compound spellings and $\Sexpr{nice.int(sum(concordance$Separate))}$ separate spellings, which results in a total sample size of $\Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))}$ tokens.

For each N+V unit in the sample, the following variables were annotated automatically: (i) the verb lemma, (ii) the noun lemma, and (iii) the overall frequency in the corpus.
The morphosyntactic contexts could be annotated semi-automatically, because separate queries were executed for each context anyway.
Additionally, we manually coded all $\Sexpr{nrow(concordance)}$ N+V units for the relation that holds between the verb and the noun.
The codes used in clear-cut cases were \textit{Argument} and \textit{Oblique}.
For $\Sexpr{length(which(concordance$Relation=="Undetermined"))}$ units, both relations were conceivable, and those cases were coded as \textit{Undetermined}.
% This class is illustrated by \textit{Daumenlutschen} (``thumb sucking''), which could correspond to either the paraphrase in % (\ref{ex:daumenlutschen-a}) or the one in (\ref{ex:daumenlutschen-b}).
%
% \begin{exe}
%   \ex\begin{xlist}
%     \ex\gll [den Daumen]\Sub{NP_{Acc}} lutschen \\
%     the thumb suck\\\label{ex:daumenlutschen-a}
%     \ex\gll [am Daumen]\Sub{PP} lutschen\\
%     {on the} thumb suck\\\label{ex:daumenlutschen-b}
%   \end{xlist}\label{ex:daumenlutschen}
% \end{exe}

The data thus obtained were analysed in two ways.
First, we report the results of a collexeme analysis in Section~\ref{sub:results2associationastrengths}, which quantifies how strongly individual N+V units tend to be written as one word or two words.
Second, in Section~\ref{sub:results1multilevelmodel} we report a full statistical model of the alternation.

\subsection{Results: association strengths}
\label{sub:results2associationastrengths}

In this section, we report an analysis of the item-specific affinities of N+V units towards univerbation.
The method we use is similar to collocation analysis (see \citealt{Evert2008} for an overview) and derives from Collostructional Analysis \parencite{StefanowitschGries2003}.
More specifically, the method is called \textit{collexeme analysis} \parencite{StefanowitschGries2009}.%
\footnote{See also \concealed{\citet{SchaeferPankratz2018}} and \concealed{\citet{Schaefer2019a}} for similar uses of this method.}

Our goal was to quantify how strongly each N+V unit tends towards univerbation vis-a-vis all other N+V units.
Thus, we need to compare the counts of cases with and without univerbation of the unit in question with the total counts for all other N+V units.
Such comparisons must be made relative to the overall number of the specific N+V unit as well as the number of all other units.
The counts needed for each N+V unit are nicely summarised in a 2$\times$2 contingency table as shown in Table~\ref{tab:associationsexplained}.

\begin{table}[!htbp]
  \begin{tabular}{lcc}
  \toprule
  & Compound spelling & Disjunct spelling \\
  \midrule
  Specific N+V unit   & $c_{11}$ & $c_{21}$ \\
  All other N+V units & $c_{21}$ & $c_{22}$ \\
  \bottomrule
  \end{tabular}
  \caption{2\CM{\times}2 contingency table as used in the calculation of the strengths of the associations of N+V units with univerbation}
  \label{tab:associationsexplained}
\end{table}

With these counts, we are able to quantify how strongly the proportions in the first row differ from those in the second row, and there is a range of statistical measures that assess the magnitude of this difference.
For example, one could use odds ratios or effects strengths from frequentist statistical tests.%
\footnote{P-values from frequentist statistical tests are measures of evidence, not effect strength, and therefore are not appropriate in such situations \parencite{SchmidKuechenhoff2013,KuechenhoffSchmid2015}, although they were used in early Collostructional Analysis.
However, even Collostructional Analysis is now often used with measures of effect strength \parencite{Gries2015b}.}
We chose Cramér's $v$ derived from standard $\chi^2$ scores ($v=\sqrt{\chi^2/n}$).
The $v$ measure quantifies for each individual N+V unit how strongly its observed counts (cells $c_{11}$ and $c_{21}$) deviate from the counts that we would expect if there were no difference between this unit and all other N+V units (cells $c_{21}$ and $c_{22}$) with respect to their tendency to univerbate.
Since Cramér's $v$ normalises the $\chi^2$ scores to the range between 0 and 1, it allows us to compare analyses where the sample sizes differ.
In itself, $v$ does not tell us whether the deviation is negative (for a N+V unit with fewer than average compound spellings) or positive (for a N+V unit with more than average compound spellings).
The information about the direction of the deviation is added by multiplying $v$ with the sign of the upper left cell of the residual table of the $\chi^2$ test.
Thus, the signed Cramér's $v$ measures how strongly individual N+V units are attracted or repelled by univerbation (positive and negative values, respectively).
Measures with such properties are often called `attraction strengths' or `association scores'.

<<associationsall, results="asis", fig.showtext = TRUE, fig.pos="htbp", fig.height=3, fig.width=6, fig.cap=paste0("Density estimate of the distributions of the \\CM{", length(which(!is.na(concordance$all.assocs))), "} association scores by the two semantic relations; the x-axis was truncated at -0.05 and 0.05 where the curves are essentially flat")>>=

### Plot distributions of association measures.
density.opts <- list(lwd = 2)
par(family = "Fira Sans", mar=c(2.5, 2.5, 0.5, 2.5))

plot(density(na.omit(concordance[which(concordance$Relation=="Adjunct"), "all.assocs"])),
     axes = F, xlab = "",
     xlim = c(-0.05, 0.05),
     lwd = density.opts$lwd,
     col = "black", lty = 1,
     main = ""
)
lines(density(na.omit(concordance[which(concordance$Relation=="Object"), "all.assocs"])),
      lwd = density.opts$lwd,
      col = "darkgreen", lty = 2
)

axis(1, at = c(-0.05, 0.05), labels=rep("",2), lwd.ticks=0)
axis(1, at = seq(-0.05, 0.05, 0.025), lwd = 0, lwd.ticks=1)
axis(2)
legend("topleft", legend = c("Oblique", "Argument"), bty = "n",
       lwd = density.opts$lwd, col = c("black", "darkgreen"), lty = c(1,2)
       )
@

We calculated the signed $v$ for each of the $\Sexpr{length(concordance$all.assocs)}$ N+V units.
The distribution of these scores is plotted in the form of a density estimate in Figure~\ref{fig:associationsall}.%
\footnote{As expected, it approximates a scaled symmetric $\chi^2$ distribution with $df=1$ squashed between $-1$ and $1$.}
The graph shows the distribution of the attraction strengths for N+V units with argument and oblique relations separately.
While there is variation in both directions in both cases, the argument relation tends more towards disjunct spelling (lower\slash more negative scores), and the oblique relation favours compound spelling more (higher\slash more positive scores).
The number of units close to $0$ (\ie without a clear tendency) is notable with the argument relation.
For example, a N+V unit strongly attracted by univerbation is \textit{Zeitreisen} (`time travel', oblique relation) with an attraction score of $\Sexpr{nice.float(concordance[which(concordance$Compound=="Zeitreisen"), "all.assocs"])}$.
An example with a strong tendency against univerbation is \textit{Fehlermachen} (`mistake make', argument relation) with an attraction score of $\Sexpr{nice.float(concordance[which(concordance$Compound=="Fehlermachen"), "all.assocs"])}$.
Finally, \textit{Haareschneiden} (`hair cut', argument relation) shows no clear tendency towards or against univerbation, having an attraction score of $\Sexpr{nice.float(concordance[which(concordance$Compound=="Haareschneiden"), "all.assocs"])}$.
The results of the association analysis will be corroborated by the subsequent analysis in Section~\ref{sub:results1multilevelmodel}, and we will use the attraction scores to control for item-specific tendencies in the experiment in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}.

\subsection{Results: full statistical model}
\label{sub:results1multilevelmodel}

In this section, we present the parameter estimates for a binomial multilevel model (or generalised linear mixed model, GLMM) which models the relevant factors influencing writers' choice of the compound and the disjunct spelling.%
\footnote{See \concealed{\citet{Schaefer2020a}} for an overview of the method and our philosophy in modelling.}
The results of the method used in Section~\ref{sub:results2associationastrengths} and the GLMM presented here converge.
However, the GLMM has a more standard interpretation and allows for finer-grained data analysis.
Also, it has long been accepted that combining several methods strengthens the analysis when the results converge (\egg \citealt{ArppeJaervikivi2007}).

Given the grand total of $\Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))}$ observations in the sample (see Section~\ref{sub:choiceofcorpussamplingandannotation}), we will completely refrain from an interpretation of the GLMM in terms of frequentist inferential statistics.
For samples of such magnitude in data-driven approaches, frequentist significance tests are the wrong tool, because it is so easy to achieve significance with such large sample sizes that conclusions based on this criterion become practically meaningless.
Therefore, we provide standard likelihood ratio confidence intervals for parameter estimates and prediction intervals for conditional modes as an approximate measure quantifying the precision of the parameter estimates and predictions.
The models we specify reflect theoretically motivated decisions, and we therefore reject all types of model selection by means of step-up or step-down procedures.

As argued in Section~\ref{sec:thestatusofnounverbunitsingerman}, we expect the probability of the univerbation of N+V units to depend on the morphosyntactic context, the relation holding between the verb and the noun, and on the specific N+V unit (a lexical tendency).
Accordingly, the response variable was chosen to be the proportion of compound spellings among all the spellings of the N+V unit.
In the input data provided to the estimator, the response variable was thus a vector of $\Sexpr{nrow(concordance)}$ proportions, one for each N+V unit.%
\footnote{Binomial models can be specified in this manner \parencite[245--260]{ZuurEa2009}.
In the estimation of such models, the influence of each proportion is weighted according to the number of cases observed to calculate it.
Without the weighting, highly frequent observed proportions would have too small an influence on the estimation, and infrequent ones would have an inappropriately high influence.
In the case at hand, such a model on proportion data is also a convenient way of getting around the practical difficulties of estimating a model on the raw $\Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))}$ observations.}

The two important fixed effects in the model are the morphosyntactic context and the internal relation (see Section~\ref{sub:choiceofcorpussamplingandannotation}).
With $\Sexpr{nrow(concordance)}$ N+V units, the lexical indicator variable for the individual N+V unit should not be used as a fixed effect, because there would be too many levels (\citealt[244--247]{GelmanHill2006}, \concealed{\citealt{Schaefer2020a}}).
Thus, we specified a generalised linear mixed model with the N+V unit variable as a random effect.
In lme4 notation, the specification is shown in (\ref{eq:corpusglmmr}).

\begin{equation}
  \mathtt{Univerbation\sim (1|NVUnit)+Context+Relation} \label{eq:corpusglmmr}
\end{equation}

<<corpusglmm, results="asis">>=

concordance.glmm$Relation <- factor(concordance.glmm$Relation, levels = c("Object", "Undetermined", "Adjunct"))
concordance.glmm$Context <- factor(concordance.glmm$Context, levels = c("Infinitive", "Participle", "NP", "Progressive"))

corpus.glmm <- glmer(cbind(Joint, Separate)~Context+Relation+(1|Compound),
                     data=concordance.glmm, family=binomial,
                     na.action = na.fail, control=glmerControl(optimizer="nloptwrap2", optCtrl=list(maxfun=2e5))
                     )

corpus.fixefs <- fixef(corpus.glmm)
corpus.confints <- confint(corpus.glmm)
# corpus.confints <- cbind(corpus.fixefs, corpus.fixefs) # Fake CIs for quick recompiles.
corpus.glmm.r2 <- r.squaredGLMM(corpus.glmm)
@

<<corpusglmmreport, results="asis">>=

# Helper function.
format.ranef <- function(glmm, ranef) {
  require(lme4)
  .vcov   <- as.data.frame(VarCorr(glmm))
  list(Name = ranef, Intercept = .vcov[which(.vcov$grp == ranef), "vcov"], sd = .vcov[which(.vcov$grp == ranef), "sdcor"])
}

# Build the table.
corpus.ct <- cbind(corpus.fixefs, corpus.confints[2:7,])
colnames(corpus.ct) <- c("Estimate", "CI low", "CI high")
ranef.nv <- format.ranef(corpus.glmm, "Compound")
corpus.r2.txt <- paste0("Nakagawa \\& Schielzeth's \\CM{R^2_m=", nice.float(corpus.glmm.r2[1,1]), "} and \\CM{R^2_c=", nice.float(corpus.glmm.r2[1,2]), "}")
ranef.txt <- paste0("Random effect for N+V lemma: \\CM{Intercept=", nice.float(ranef.nv$Intercept), "}, \\CM{sd=", nice.float(ranef.nv$sd),
                    "}. ")
corpus.ctxt <- xtable(corpus.ct, digits = 3,
                    caption = paste0("Coefficient table for the binomial GLMM modelling the corpus data with 95\\% profile likelihood ratio confidence intervals. Weighting was used to account for the bias in models on proportion data. The intercept models the levels Context~=~Infinitive and Relation~=~Argument. ", ranef.txt, corpus.r2.txt),
                    label = "tab:corpusglmm")

# Experimental function to fix variable names.
lme4.pretty <- function(s) {
  s <- gsub("([a-z])([A-Z])", "\\1 = \\2", s)
  s <- gsub("Object", "Argument", s)
  s <- gsub("Adjunct", "Oblique", s)
  s <- gsub("Intercept", "Context = Infinitive, Relation = Argument", s)
  s
}

# Print the table.
print(corpus.ctxt,
      include.rownames=T,
      floating = T,
      table.placement = '!htbp',
      booktabs = T,
      scalebox = 1,
      hline.after = c(-1,0,1,4,6),
      math.style.negative = T,
      sanitize.text.function = lme4.pretty
)
@

The estimated parameters of the model are given in Table~\ref{tab:corpusglmm}.
Additionally, effect plots for \textit{Context} and \textit{Relation} are given in Figure~\ref{fig:corpuseffects}.%
\footnote{Effect plots for binomial GLM(M)s \parencite{FoxWeisberg2018} plot the probability of the outcome across values of a regressor assuming default values for all other regressors.
While model coefficients in binomial (and other) models have no direct interpretation in terms of probability, effect plots allow a more intuitive interpretation in terms of changes in probability.
For better interpretability, the y-axis in effect plots is plotted on the scale of the linear predictor (logits in a GLMM), with labels added on the scale of the response (probabilities derived via the inverse logit link function in GLMMs).
See \citet[14]{FoxWeisberg2018} for an illustrative example.
This is why the labels of the y axes are never aligned across plots.}
As expected, the prototypically verbal contexts (infinitives and participles) are associated with a low probability of compound spelling (the infinitive is on the intercept, which is estimated at $\Sexpr{nice.float(corpus.ct["(Intercept)","Estimate"])}$, and participles have a coefficient of $\Sexpr{nice.float(corpus.ct["ContextParticiple","Estimate"])}$).
NPs and progressives as prototypically nominal contexts clearly favour compound spelling (coefficients of $\Sexpr{nice.float(corpus.ct["ContextNP","Estimate"])}$ and $\Sexpr{nice.float(corpus.ct["ContextProgressive","Estimate"])}$, respectively).
Both the coefficients and the effect plot (right panel in Figure~\ref{fig:corpuseffects}) show a low probability of compound spelling when an argument relation holds between the verb and the noun (on the intercept), and a high probability when the relation is oblique (coefficient $\Sexpr{nice.float(corpus.ct["RelationAdjunct","Estimate"])}$).
The undetermined cases are in between the two clear-cut cases (coefficient $\Sexpr{nice.float(corpus.ct["RelationUndetermined","Estimate"])}$).

<<corpuseffects, results="asis", fig.showtext=TRUE, fig.pos="!htbp", fig.height=4, fig.cap="Effect plots for the regressor encoding the morphosyntactic context of the N+V unit and the regressor encoding the syntactic relation within the N+V unit in the GLMM modelling the corpus data">>=
ef.context <- effect("Context", corpus.glmm, KR = T)
ef.context$variables$Context$levels <- c("Inf.", "Part.", "NP", "Prog.")
levels(ef.context$x$Context) <- c("Inf.", "NP", "Part.", "Prog.")
corpus.glmm.fx.context <- plot(ef.context,
                              rug = F,
                              colors = c("black", "black"),
                              main = "",
                              ylab = "Probability of univerbation",
                              xlab = "Morphosyntactic context"
                              #,ylim = c(0,1)
                              #,rescale.axis = F
                              )

ef.rel <- effect("Relation", corpus.glmm, KR = T)
ef.rel$variables$Relation$levels <- c("Argument", "Undetermined", "Oblique")
levels(ef.rel$x$Relation) <- c("Oblique", "Argument", "Undetermined")
corpus.glmm.fx.relation <- plot(ef.rel,
                                rug=F, colors = c("black", "black"),
                                main="",
                                ylab="Probability of univerbation",
                                xlab="Semantic relation"
                                #,ylim = c(0,1)
                                #,rescale.axis = F
                                )
grid.arrange(corpus.glmm.fx.context, corpus.glmm.fx.relation, ncol=2)
@


<<ranefplotsetnumber, results="hide">>=
ranefplot.num=20
@

Given the narrow confidence intervals and the high marginal measure of determination $R^2_m=\Sexpr{nice.float(corpus.glmm.r2[1,1])}$, we consider the hypotheses regarding fixed effects to be well corroborated by the data.
The differences between specific N+V units already shown in Section~\ref{sub:results2associationastrengths} show up in the model as the residual variance in the random effects (in the form of the conditional modes).
The conditional modes are centred around $\Sexpr{nice.float(ranef.nv$Intercept)}$ with a standard deviation of $\Sexpr{nice.float(ranef.nv$sd)}$.
The relatively high standard deviation is a sign that there is considerable variation across the individual N+V units.
Furthermore, the conditional $R^2_c$ is as high as $\Sexpr{nice.float(corpus.glmm.r2[1,2])}$.
This is commonly interpreted as saying that the fixed effects and the idiosyncratic effect of concrete N+V units almost fully explain the variance in the data.
A random selection of $\Sexpr{nice.int(ranefplot.num)}$ conditional modes, which illustrates the relevance of lexical idiosyncrasies through obvious differences with mostly very narrow prediction intervals, is shown in Figure~\ref{fig:corpusranefs}.
The individual N+V unit thus plays a major role in writers' tendency to univerbate N+V units, which matches the results from Section~\ref{sub:results2associationastrengths}.

<<corpusranefs, results="asis", fig.showtext=TRUE, fig.pos="!htbp", fig.cap="A random selection of conditional modes with 95\\% prediction intervals for the levels of the random effect in the GLMM modelling the corpus data">>=
set.seed(3478)
par(family = "Fira Sans")
corpus.ranef.selection <- ranef.plot(corpus.glmm, effect = "Compound", number = 20)
@

