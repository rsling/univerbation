% !Rnw root = ../nvuniverbation.Rnw
<<setupcorpus, cache=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\section{Corpus-based analysis of the usage of verb-noun units}
\label{sec:corpusbasedanalysisoftheusageofvnunits}

\subsection{Design and choice of corpus}
\label{sub:designandchoiceofcorpus}

The goal of the corpus study was to assess (i) which V+N units exist in written German usage, and (ii) how strongly they are attracted by the univerbation effect.
The operationalisation of question (ii) relied on the fact that the major graphemic principles in German are clear and dominant, and that they are both deeply rooted in diachrony and well entrenched in writers' usage.
The relevant major principle for the present study was compound spelling of words, which we took as an indication that in the grammars of the writer the compounded words had single-word status.

Research questions (i) and (ii) -- as opposed to the  -- are clearly not driven by strong hypotheses derived from theory, and we consequently adopted a data-driven approach with a post-hoc interpretation of the results. %
\footnote{The results obtained from the corpus were also used in the choice of the stimuly for the experiment reported in Section~\ref{sec:elicitedproductionofnounverbunitsinwrittenlanguage}.}
Hence, we needed to extract (close to) \textit{all} relevant N+V units from an ideally very large and varied corpus as a first step.
In a second step, we had to count their occurrences in compound and separate spelling in the relevant morphosyntactic contexts enumerated in Section~\ref{sec:theformandhistoryofnounverbunitsingerman}, viz.\ as the heads of noun phrases, \textit{am} progressives, as participles in analytical verb forms, and as infinitives in a range of verbal constructions (for example with modal verbs).

Clearly, a large corpus with rich morphological and morphosyntactic annotations containing texts written in a broad variety of registers and styles (including ones with low normative pressure) was required.
We chose the DECOW16B corpus \parencite{SchaeferBildhauer2012} because it fulfils all the abovementioned criteria.%
\footnote{\url{https://www.webcorpora.org}}
Much like the SketchEngine corpora \parencite{KilgarriffEa2014}, the COW corpora contain web documents from recent years.
However, the German DECOW (containing 20.5 billion tokens in 808 million sentences and 17.1 million documents) offers a much wider range of annotations compared to SketchEngine corpora, including morphological annotations and serveal levels of syntactic annotation (dependencies and topological parses).
For our purpose, the fully internal analysis of nominal compounds described in \textcite{SchaeferPankratz2018} was particularly of interest.
It allows for searches of roots within nominal compounds.
For example, we could query compounds with a deverbal head such as \textit{Zeitnehmen} (`time taking').
Furthermore, the interface offered by the creators of the COW corpora allows for automated queries controlled by Python scripts using the open-source \textit{SeaCOW} interface.%
\footnote{\url{https://github.com/rsling/seacow}}
The scripts we used to make the queries are released on a cureted open-data server along with all data as well as the \LaTeX, knitr, and R scripts created in the writing of this paper.%
\footnote{The DOI of the data set will be revealed in the accepted version of this paper.}

\subsection{Sampling and annotation}
\label{sub:samplingandannotation}

The first step of the implementation of the corpus study was the generation of a list of actually occurring N+V units.
We obtained such a list by querying for compounds with a nominal non-head and a deverbal head.
(See the scripts available under the abovementioned DOI for concrete queties and further details.)
The rationale behind this approach was that any N+V unit of interest should occur at least one in compound spelling as a fully nominalised compound.
Since this step relied on automatic annotation, the results contained erroneous results, which we cleaned through manual annotation.
The resulting list contained \Sexpr{nrow(concordance)} N+V units.

In the second step, we created lists of all relevant inflectional forms of the verb in each V+N unit and used these to query all possible compound and separate spellings (including variance in capitalisation) of each of the \Sexpr{nrow(concordance)} N+V units.
In total, \Sexpr{nice.int(35*nrow(concordance))} queries were executed to create the final data set used here, a number which clearly demonstrates the necessity of script-based corpus access in data-driven methods.
The queries were matched by \Sexpr{nice.int(sum(concordance$Joint))} compound spellings and \Sexpr{nice.int(sum(concordance$Separate))} separate spellings, which results in a total sample size of \Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))}.%
\footnote{Notice that two highly frequent N+V units were excluded because they could be considered outliers, having an overly strong tendency to be used in compound spelling.
They are \textit{Teilnehmen} (``taking part'') and \textit{Ma√ünehmen} (``taking measure'').}

For each N+V unit in the sample, the following variables were annotated automatically: (i) the verb, (ii) the noun, (iii) whether a linking element is used in the use as a full noun, (iv) the overall frequency in the corpus.
Additionally, we manually coded all \Sexpr{nrow(concordance)} N+V units for the relation holding between the verb and the noun (see Section~\ref{sec:theformandhistoryofnounverbunitsingerman}).
The codes used in clearcut cases were \textit{Object} (\Sexpr{length(which(concordance$Relation=="Object"))} units) and \textit{Adjunct} (\Sexpr{length(which(concordance$Relation=="Adjunct"))} units).
For \Sexpr{length(which(concordance$Relation=="Undetermined"))} units, both relations were conceivable, and those cases were coded as \textit{Undetermined}.
This class is illustrated by \textit{Daumenlutschen} (``thumb sucking''), which could be paraphrased as either (\ref{ex:daumenlutschen-a}) or (\ref{ex:daumenlutschen-b}).

\begin{exe}
  \ex\begin{xlist}
    \ex\gll das Lutschen des Daumens\\
    the sucking of the thumb\\\label{ex:daumenlutschen-a}
    \ex\gll das Lutschen am Daumen\\
    the sucking {on the} thumb\\\label{ex:daumenlutschen-b}
  \end{xlist}\label{ex:daumenlutschen}
\end{exe}


\subsection{Modelling the corpus data}
\label{sub:modellingthecorpusdata}

In this section, we present the parameter estimates (and predictions of conditional modes) for a multilevel generalised model (or generalised linear mixed model, GLMM) which models the -- in our view -- the relevant factors influencing speakers' choice of the compound and the separate spelling.%
\footnote{See \parencite{Schaefer2020} for an overview of the method and our philosophy in modelling.}
Given the grand total of \Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))} observations in the sample (see Section~\ref{sec:samplingandannotation}), we will completely refrain of using inferential statistics per se.
For samples of such magnitude in data-driven approaches, frequentist significance tests are the wrong tool.
Bayesian methods reliably converge with frequentist methods at this sample size.
Therefore, we provide simple standard frequentist confidence intervals for parameter estimates and prediction intervals for conditional modes as an approximate measure quantifying the precision of the parameter estimates.
The models we specify reflect theoretically motivated decisions, and we therefore reject all types of model selection by means of step-up or step-down procedures.

As argued in Section~\ref{sec:thestatusofounverbunits}, we expect the probability of the univerbation of N+V units to depend on the relation holding between the verb and the noun, the presence of absence of a linking element in the nominal compound (as a marker of a stronger reconceptualisation) and on the specific N+V unit (a lexical tendency).
Accordingly, the response variable was chosen to be the proportion of compound spellings among all spellings of the N+V unit.
The input data frame to the estimator was thus a table of \Sexpr{nrow(concordance)} proportions, one for each N+V unit.%
\footnote{Binomial models can be specified in this manner \parencite[245--260]{ZuurEa2009}.
In the estimation of such models, the influence of each proportion is weighted according to the number of cases observed to calculate it.
Without the weighting, highly frequent N+V units would have too little influence on the estimation of the model, and infrequent ones would have an inappropriately high influence.
In the case at hand, such a model on proportion data is also a convenient way of getting around difficulties of estimating a model on the raw \Sexpr{nice.int(sum(concordance$Joint)+sum(concordance$Separate))} observations.}
We specified three regressors.
As there is a huge number of \Sexpr{nrow(concordance)} N+V units, the lexical indicator variable for the individual N+V unit should not be used as a fixed effect \parencite[244--247]{GelmanHill2006}.
Therefore, we specified a generalised linear model with the N+V unit variable as a random effect.
The variables encoding the internal relation and the presence\slash absence of a linking element are nested inside the levels of the random effect, and they are therefore threated as second-level fixed effects in a multilevel model.%
\footnote{In R notation, the specification is thus: \texttt{Proportion\~Relation+Link+(1|NVUnit)}.}
The estimated parameters of the model are given in Table~\ref{tab:corpusglmm}.

<<corpusglmm, results="asis">>=

concordance.glmm$Relation <- factor(concordance.glmm$Relation, levels = c("Object", "Undetermined", "Adjunct"))
concordance.glmm$Context <- factor(concordance.glmm$Context, levels = c("Infinitive", "Participle", "NP", "Progressive"))
concordance.glmm$FullLink <- concordance.glmm$Link
concordance.glmm$Link <- concordance.glmm$Linkbinary

corpus.glmm <- glmer(cbind(Joint, Separate)~Context+Relation+Link+(1|Compound),
                     data=concordance.glmm, family=binomial,
                     na.action = na.fail, control=glmerControl(optimizer="nloptwrap2", optCtrl=list(maxfun=2e5))
                     )

corpus.fixefs <- fixef(corpus.glmm)
corpus.confints <- confint(corpus.glmm)
corpus.glmm.r2 <- r.squaredGLMM(corpus.glmm)
@

<<corpusglmmreport, results="asis">>=

# Helper function.
format.ranef <- function(glmm, ranef) {
  require(lme4)
  .vcov   <- as.data.frame(VarCorr(glmm))
  list(Name = ranef, Intercept = .vcov[which(.vcov$grp == ranef), "vcov"], sd = .vcov[which(.vcov$grp == ranef), "sdcor"])
}

# Build the table.
corpus.ct <- nice.float(cbind(corpus.fixefs, corpus.confints[1:7,]))
colnames(corpus.ct) <- c("Estimate", "CI low", "CI high")
ranef.nv <- format.ranef(corpus.glmm, "Compound")
corpus.r2.txt <- paste0("Nakagawa \\& Schielzeth's \\CM{R^2_m=", nice.float(corpus.glmm.r2[1,1]), "} and \\CM{R^2_c=", nice.float(corpus.glmm.r2[1,2]), "}")
ranef.txt <- paste0("Random effect for V+N lemma: \\CM{Intercept=", nice.float(ranef.nv$Intercept), "}, \\CM{sd=", nice.float(ranef.nv$sd),
                    "}")
corpus.ctx <- xtable(corpus.ct,
                    caption = paste0("Coefficient table for the binomial GLMM modelling the corpus data with 95\\% confidence intervals. The response is for each N+V unit in each morphosyntactic context (\\CM{N_{u}=", nrow(concordance.glmm), "} derived from a total number of observations (\\CM{N_{obs}=", nice.int(sum(concordance$Joint)+sum(concordance$Separate)),"}) the proportion of compound spellings among all of its spellings. Weighting was used to account for the bias in models on proportion data. ", ranef.txt, ". The intercepts model the fixed effects Relation=Object and Link=No. " , corpus.r2.txt),
                    label = "tab:corpusglmm")

# Print the table.
print(corpus.ctx,
      include.rownames=T,
      floating = T,
      table.placement = 'h!',
      booktabs = T,
      scalebox = 1,
      sanitize.text.function = function(x){x},
)
@

As expected,


<<effectcontext, results="asis", fig.showtext=TRUE, fig.pos="htbp", fig.height=5, fig.cap="Effect plot for the regressor encoding the morphosyntactic context of the N+V unit in the GLMM modelling the corpus data">>=
plot(effect("Context", corpus.glmm, KR = T), rug=F, colors = c("black", "darkorange"),
          main="",
          ylab="Probability of univerbation",
          xlab="Context"
)
@

<<effectrelation, results="asis", fig.showtext=TRUE, fig.pos="htbp", fig.height=5, fig.cap="Effect plot for the regressor encoding the syntactic relation within the N+V unit in the GLMM modelling the corpus data">>=
plot(effect("Relation", corpus.glmm, KR = T), rug=F, colors = c("black", "darkorange"),
          main="",
          ylab="Probability of univerbation",
          xlab="Relation"
)
@


<<corpusranefs, results="asis", fig.showtext=TRUE, fig.pos="htbp", fig.cap="A random selection of conditional modes with 95\\% prediction intervals for the levels of the random effect in the GLMM modelling the corpus data">>=
set.seed(3478)
par(family = "Fira Sans")
corpus.ranef.selection <- ranef.plot(corpus.glmm, effect = "Compound", number = 20)
@



\subsection{Association strengths}
\label{sub:associationastrengths}



<<associationsall, results="asis", fig.showtext = TRUE, fig.pos="htbp", fig.height=4, fig.width=4, fig.cap=paste0("Density estimate of the distribution of the overall association scores (across all morphosyntactic conditions) with \\CM{n=", length(which(!is.na(concordance$all.assocs))), "}")>>=

### Plot distributions of association measures.
density.opts <- list(lwd = 2)
par(family = "Fira Sans")

plot(density(na.omit(concordance$all.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = ""
  )
axis(1)
axis(2)
@

<<associationssingle, results="asis", fig.showtext = TRUE, fig.pos="htbp", fig.cap="Density estimates of the distribution of the association scores in the specific morphosyntactic conditions. Because of some undefined scores the sample sizes \\CM{n} vary">>=

### Plot distributions of association measures.
density.opts <- list(lwd = 2)
par(mfrow=c(2,2), family = "Fira Sans")

plot(density(na.omit(concordance$np.all.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = paste0("NPs (n=", length(which(!is.na(concordance$np.all.assocs))), ")")
  )
axis(1)
axis(2)

plot(density(na.omit(concordance$prog.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = paste0("Progressives (n=", length(which(!is.na(concordance$prog.assocs))), ")")
  )
axis(1)
axis(2)

plot(density(na.omit(concordance$particip.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = paste0("Participles (n=", length(which(!is.na(concordance$particip.assocs))), ")")
  )
axis(1)
axis(2)

plot(density(na.omit(concordance$infzu.assocs)),
  axes = F, xlab = "", xlim = c(-0.15, 0.15),
  lwd = density.opts$lwd,
  col = "black", lty = 1,
  main = paste0("Infinitives (n=", length(which(!is.na(concordance$infzu.assocs))), ")")
  )
axis(1)
axis(2)
@


<<corpusassoctablehi, results="asis">>=
corpus.assoc.sel <- format.assocs(df = concordance, col = 'all.assocs', show.results = "all",
                               show.cols = c("Compound", "all.assocs", "Relation"),
                               num = 10, cx = "All contexts", effect = "UNIVERBATION", freq.cutoff = 0.2)
colnames(corpus.assoc.sel) <- c("V+N Unit", "Association", "Relation")
corpus.assoc.sel$Association <- nice.float(corpus.assoc.sel$Association)

corpus.assoc.hi <- xtable(corpus.assoc.sel[1:10,],
                   caption = paste0("Top ten V+N units with a strong tendency for univerbation"),
                   label = "tab:assochi")
# Print the table.
print(corpus.assoc.hi,
      include.rownames=F,
      floating = T,
      table.placement = 'h!',
      booktabs = T,
      scalebox = 1,
      sanitize.text.function = function(x){x},
)
@


<<corpusassoctablenull, results="asis">>=
corpus.assoc.null <- xtable(corpus.assoc.sel[11:20,],
                            caption = paste0("Top ten V+N units without any tendency for or against univerbation"),
                            label = "tab:assocnull")
# Print the table.
print(corpus.assoc.null,
      include.rownames=F,
      floating = T,
      table.placement = 'h!',
      booktabs = T,
      scalebox = 1,
      sanitize.text.function = function(x){x},
)
@

<<corpusassoctablelo, results="asis">>=
corpus.assoc.lo <- xtable(corpus.assoc.sel[30:21,],
                          caption = paste0("Top ten V+N units with a strong tendency against univerbation"),
                          label = "tab:assoclo")
# Print the table.
print(corpus.assoc.lo,
      include.rownames=F,
      floating = T,
      table.placement = 'h!',
      booktabs = T,
      scalebox = 1,
      sanitize.text.function = function(x){x},
)
@
